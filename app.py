"""
Streamlit Web App for Sign Language to Text and Speech Conversion
Real-time hand gesture recognition using webcam
"""

import streamlit as st
import cv2
import numpy as np
from keras.models import load_model
from cvzone.HandTrackingModule import HandDetector
import os
import math
from gtts import gTTS
import tempfile
import base64

# Page config
st.set_page_config(
    page_title="Sign Language Recognition",
    page_icon="ü§ü",
    layout="wide"
)

# Paths
MODEL_PATH = os.path.join(os.path.dirname(__file__), 'cnn8grps_rad1_model.h5')
WHITE_IMAGE_PATH = os.path.join(os.path.dirname(__file__), 'white.jpg')

# Initialize session state
if 'sentence' not in st.session_state:
    st.session_state.sentence = ""
if 'current_char' not in st.session_state:
    st.session_state.current_char = ""
if 'prev_char' not in st.session_state:
    st.session_state.prev_char = ""
if 'char_count' not in st.session_state:
    st.session_state.char_count = 0

# Load model
@st.cache_resource
def load_gesture_model():
    if not os.path.exists(MODEL_PATH):
        st.error(f"Model file not found at {MODEL_PATH}. Please download it using download_assets.py")
        return None
    return load_model(MODEL_PATH)

# Helper functions
def distance(x, y):
    return math.sqrt(((x[0] - y[0]) ** 2) + ((x[1] - y[1]) ** 2))

def process_hand_gesture(white_skeleton, pts, w, h):
    """Process hand landmarks and return predicted character"""
    pl = [0, 0]  # Placeholder for prediction logic
    
    # Add gesture classification logic here (simplified)
    # This is a placeholder - actual logic from final_pred.py would go here
    return "A"  # Placeholder

def create_white_skeleton(image, handz, hd2, w, h, offset=29):
    """Create skeleton image from hand landmarks"""
    white = np.ones((400, 400, 3), np.uint8) * 255
    
    if handz:
        hand = handz[0]
        pts = hand['lmList']
        
        os_val = ((400 - w) // 2) - 15
        os1 = ((400 - h) // 2) - 15
        
        # Draw hand skeleton
        for t in range(0, 4, 1):
            cv2.line(white, (pts[t][0] + os_val, pts[t][1] + os1), 
                    (pts[t + 1][0] + os_val, pts[t + 1][1] + os1), (0, 255, 0), 3)
        for t in range(5, 8, 1):
            cv2.line(white, (pts[t][0] + os_val, pts[t][1] + os1), 
                    (pts[t + 1][0] + os_val, pts[t + 1][1] + os1), (0, 255, 0), 3)
        for t in range(9, 12, 1):
            cv2.line(white, (pts[t][0] + os_val, pts[t][1] + os1), 
                    (pts[t + 1][0] + os_val, pts[t + 1][1] + os1), (0, 255, 0), 3)
        for t in range(13, 16, 1):
            cv2.line(white, (pts[t][0] + os_val, pts[t][1] + os1), 
                    (pts[t + 1][0] + os_val, pts[t + 1][1] + os1), (0, 255, 0), 3)
        for t in range(17, 20, 1):
            cv2.line(white, (pts[t][0] + os_val, pts[t][1] + os1), 
                    (pts[t + 1][0] + os_val, pts[t + 1][1] + os1), (0, 255, 0), 3)
        
        # Connect palm
        cv2.line(white, (pts[5][0] + os_val, pts[5][1] + os1), 
                (pts[9][0] + os_val, pts[9][1] + os1), (0, 255, 0), 3)
        cv2.line(white, (pts[9][0] + os_val, pts[9][1] + os1), 
                (pts[13][0] + os_val, pts[13][1] + os1), (0, 255, 0), 3)
        cv2.line(white, (pts[13][0] + os_val, pts[13][1] + os1), 
                (pts[17][0] + os_val, pts[17][1] + os1), (0, 255, 0), 3)
        cv2.line(white, (pts[0][0] + os_val, pts[0][1] + os1), 
                (pts[5][0] + os_val, pts[5][1] + os1), (0, 255, 0), 3)
        cv2.line(white, (pts[0][0] + os_val, pts[0][1] + os1), 
                (pts[17][0] + os_val, pts[17][1] + os1), (0, 255, 0), 3)
        
        # Draw landmarks
        for i in range(21):
            cv2.circle(white, (pts[i][0] + os_val, pts[i][1] + os1), 2, (0, 0, 255), 1)
    
    return white

def predict_gesture(model, white_skeleton, pts):
    """Predict gesture from skeleton image"""
    if model is None:
        return "?"
    
    # Preprocess
    white_skeleton = white_skeleton.reshape(1, 400, 400, 3)
    prob = np.array(model.predict(white_skeleton, verbose=0)[0], dtype='float32')
    ch1 = np.argmax(prob, axis=0)
    
    # Apply gesture classification rules (simplified from final_pred.py)
    # This would include all the conditional logic for distinguishing similar gestures
    
    # Map to character groups
    char_groups = {
        0: ['A', 'E', 'M', 'N', 'S', 'T'],
        1: ['B', 'D', 'F', 'I', 'K', 'R', 'U', 'V', 'W'],
        2: ['C', 'O'],
        3: ['G', 'H'],
        4: ['L'],
        5: ['P', 'Q', 'Z'],
        6: ['X'],
        7: ['Y', 'J']
    }
    
    # Return first char from predicted group (simplified)
    if ch1 in char_groups:
        return char_groups[ch1][0]
    
    return chr(ch1 + 65) if ch1 < 26 else "?"

def text_to_speech(text):
    """Convert text to speech and return audio file path"""
    if not text.strip():
        return None
    
    try:
        tts = gTTS(text=text, lang='en', slow=False)
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:
            tts.save(fp.name)
            return fp.name
    except Exception as e:
        st.error(f"Text-to-speech error: {e}")
        return None

def autoplay_audio(file_path):
    """Auto-play audio in Streamlit"""
    with open(file_path, "rb") as f:
        data = f.read()
        b64 = base64.b64encode(data).decode()
        md = f"""
            <audio autoplay="true">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
            """
        st.markdown(md, unsafe_allow_html=True)

# Main UI
st.title("ü§ü Sign Language to Text & Speech")
st.markdown("Real-time American Sign Language recognition using webcam")

# Sidebar
with st.sidebar:
    st.header("Controls")
    
    if st.button("üóëÔ∏è Clear Sentence"):
        st.session_state.sentence = ""
        st.session_state.current_char = ""
        st.rerun()
    
    if st.button("üîä Speak Sentence"):
        if st.session_state.sentence:
            audio_file = text_to_speech(st.session_state.sentence)
            if audio_file:
                autoplay_audio(audio_file)
                os.unlink(audio_file)
    
    st.markdown("---")
    st.markdown("### Instructions")
    st.markdown("""
    1. Allow webcam access
    2. Show hand gestures to camera
    3. Recognized characters appear automatically
    4. Build sentences and use Speak button
    """)
    
    st.markdown("---")
    st.markdown("### About")
    st.markdown("""
    This app recognizes American Sign Language (ASL) 
    hand gestures in real-time using a CNN model.
    
    **Accuracy:** ~97%
    """)

# Main content
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("üìπ Webcam Feed")
    run = st.checkbox('Start Camera', value=False)
    FRAME_WINDOW = st.image([])

with col2:
    st.subheader("üñêÔ∏è Hand Skeleton")
    SKELETON_WINDOW = st.image([])

# Display current recognition
st.markdown("---")
col_char, col_sentence = st.columns(2)

with col_char:
    st.subheader("Current Character")
    char_placeholder = st.empty()
    char_placeholder.markdown(f"<h1 style='text-align: center; color: green;'>{st.session_state.current_char or '‚Äî'}</h1>", 
                             unsafe_allow_html=True)

with col_sentence:
    st.subheader("Sentence")
    sentence_placeholder = st.empty()
    sentence_placeholder.markdown(f"<h3>{st.session_state.sentence or 'Start signing...'}</h3>", 
                                 unsafe_allow_html=True)

# Camera processing
if run:
    model = load_gesture_model()
    if model is None:
        st.stop()
    
    hd = HandDetector(maxHands=1)
    hd2 = HandDetector(maxHands=1)
    offset = 29
    
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        st.error("Cannot access webcam. Please check permissions.")
        st.stop()
    
    # Performance optimization: process every Nth frame
    frame_skip = 2  # Process every 2nd frame
    frame_count = 0
    
    while run:
        ret, frame = cap.read()
        if not ret:
            st.error("Failed to grab frame")
            break
        
        frame_count += 1
        
        # Skip frames for performance
        if frame_count % frame_skip != 0:
            continue
        
        frame = cv2.flip(frame, 1)
        hands, _ = hd.findHands(frame, draw=False, flipType=True)
        
        skeleton_img = np.ones((400, 400, 3), np.uint8) * 255
        
        if hands:
            hand = hands[0]
            x, y, w, h = hand['bbox']
            
            # Extract hand region
            image = frame[max(0, y - offset):y + h + offset, max(0, x - offset):x + w + offset]
            
            if image.size > 0:
                handz, _ = hd2.findHands(image, draw=False, flipType=True)
                
                if handz:
                    # Create skeleton
                    skeleton_img = create_white_skeleton(image, handz, hd2, w, h, offset)
                    
                    # Predict
                    pts = handz[0]['lmList']
                    predicted_char = predict_gesture(model, skeleton_img, pts)
                    
                    # Update session state
                    if predicted_char != st.session_state.prev_char:
                        st.session_state.char_count = 0
                        st.session_state.prev_char = predicted_char
                    else:
                        st.session_state.char_count += 1
                    
                    # Add to sentence if stable (shown for ~1 second)
                    if st.session_state.char_count > 10:
                        if predicted_char != " ":
                            st.session_state.sentence += predicted_char
                            st.session_state.char_count = 0
                    
                    st.session_state.current_char = predicted_char
                    
                    # Draw on frame
                    cv2.rectangle(frame, (x - offset, y - offset), 
                                (x + w + offset, y + h + offset), (0, 255, 0), 2)
                    cv2.putText(frame, f"Detected: {predicted_char}", (x, y - 10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
        
        # Display frames
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        FRAME_WINDOW.image(frame_rgb)
        SKELETON_WINDOW.image(skeleton_img)
        
        # Update displays
        char_placeholder.markdown(f"<h1 style='text-align: center; color: green;'>{st.session_state.current_char or '‚Äî'}</h1>", 
                                 unsafe_allow_html=True)
        sentence_placeholder.markdown(f"<h3>{st.session_state.sentence or 'Start signing...'}</h3>", 
                                     unsafe_allow_html=True)
    
    cap.release()

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>Built with Streamlit | <a href='https://github.com/souvik001122/Sign2TextSpeech'>GitHub</a></p>
</div>
""", unsafe_allow_html=True)
